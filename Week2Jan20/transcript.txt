#################################################################
#1.  Running MapReduce

#Goals:
# * Use a script to make running MapReduce jobs easier
# * Show what various error messages look like
# * Show a more complex use of MapReduce (change the map key and have reduce take an average)

# Started with a new VM.
# Copied the folder mrstreaming from the repository into my home directory
#   (Ideally we would just clone the repository in the VM, but the command
#    was breaking on an HTTP error, and it was easiest just to copy the directory 
#   in, because VMWare Workstation supports drag and drop
# * Create the bin directory to hold the script, which looks like this:

cat bin/stream
hdfs dfs -rm /user/training/data-output/$3/*
hdfs dfs -rmdir /user/training/data-output/$3
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -file /home/training/mrstreaming/$1/mapper.py \
    -mapper /home/training/mrstreaming/$1/mapper.py \
    -file /home/training/mrstreaming/$1/reducer.py \
    -reducer /home/training/mrstreaming/$1/reducer.py \
    -input /user/training/data-input/$2/* \
    -output /user/training/data-output/$3 \

#  Notice it takes three arguments, which fix the location of the mapper and reducer, the 
#  MapReduce input file directory, and the MapReduce output file directory.
#  Notice that having multiple input files in the HDFS input directory is fine:  MapReduce
#    will automatically iterate over all of them
#  Pay attention to the fact that /user/training is a directory in HDFS and /home/training 
#     is a directory on the VM filesystem

#  Get a big text file for test/demo purposes.  This came from a link on the streaming MapReduce
#   website.  I'm putting it in /tmp because I'm going to upload it to HDFS then I don't need it

[training@localhost ~]$ curl http://www.gutenberg.org/files/4300/4300-0.txt > /tmp/ulysses.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1549k  100 1549k    0     0   801k      0  0:00:01  0:00:01 --:--:--  879k

# And just verify it's a reasonable size and content looks reasonable

[training@localhost ~]$ head /tmp/ulysses.txt 
﻿
The Project Gutenberg EBook of Ulysses, by James Joyce

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever. You may copy it, give it away or
re-use it under the terms of the Project Gutenberg License included
with this eBook or online at www.gutenberg.org


Title: Ulysses

[training@localhost ~]$ wc /tmp/ulysses.txt 
  33230  268132 1586488 /tmp/ulysses.txt

# Put the script up in the HDFS input directory, which according to the script is 
#   -input /user/training/data-input/$2/*
# That is, we need to create data-input, then create some subdirectory to hold our text files

[training@localhost ~]$ hdfs dfs -mkdir /user/training/data-input
[training@localhost ~]$ hdfs dfs -mkdir /user/training/data-input/ulysses
[training@localhost ~]$ hdfs dfs -put /tmp/ulysses.txt /user/training/data-input/ulysses/ulysses.txt 

# And see if the content looks the same up there in HDFS
[training@localhost ~]$ hdfs dfs -cat /user/training/data-input/ulysses/ulysses.txt | head
﻿
The Project Gutenberg EBook of Ulysses, by James Joyce

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever. You may copy it, give it away or
re-use it under the terms of the Project Gutenberg License included
with this eBook or online at www.gutenberg.org


Title: Ulysses

#  Create a directory to hold the map-reduce output.
#  According to the script:  -output /user/training/data-output/$3

[training@localhost ~]$ hdfs dfs -mkdir /user/training/data-output
[training@localhost ~]$ hdfs dfs -mkdir /user/training/data-output/ulysses

# But before we run on the cluster, look at the mapper and reducer, and run some local tests

[training@localhost ~]$ cat mrstreaming/avg-word-length-by-letter/mapper.py
#!/usr/bin/env python
"""mapper.py"""

import sys
import string


for line in sys.stdin:
    for word in line.strip().split():

        # Normalize the word to lower case, then remove all characters that aren't
        # lower-case letters
        lowered = word.lower()
        filtered = filter(lambda c: 97 <= ord(c) <= 122, lowered)

        if len(filtered) > 0:
            # Just for illustration purposes, demonstrate mapper can generate any (key,value) it
            # wants to.  Key is a classification based on first letter in the word:  
            #    words beginning with 'a' to 'i' are tagged as "early"  'j' to 'r' is "middle", 
            #    and 's' to 'z' are late.  No particular reason, just to do something like see 
            #    whether 'early' words are shorter on average than 'late' and other things like that

            o = ord(filtered[0])
            if o <= 105:
                k = 'early'
            elif o <= 114:
                k = 'middle'
            elif o <= 122:
                k = 'late'
            else: 
                raise "Bad ordinal!"
            print '%s\t%s' % (k, len(filtered))

#  Test the mapper on the local machine
[training@localhost ~]$ python mrstreaming/avg-word-length-by-letter/mapper.py < /tmp/ulysses.txt | head -n 3
late	3
middle	7
early	9

# Here is the reducer.  For each key (early, middle,late) it just takes the average of the values for those
# records

[training@localhost ~]$ cat mrstreaming/avg-word-length-by-letter/reducer.py
#!/usr/bin/env python
import sys

current_label = None
current_sum = 0
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    label, fcount = line.split('\t', 1)
    #  It is a big decision whether to error, log, or ignore bad inputs!
    try:
        fcount = int(fcount)
    except ValueError:
        continue

    if current_label == label:
        current_sum += fcount
        current_count += 1
    else:
        if current_label:
            # Multipy by 1.0 needed to force real-valued division
            print '%s\t%f' % (current_label, (1.0 * current_sum)/current_count)
        current_label = label
        current_sum = fcount
        current_count = 1

if current_label == label:
    print '%s\t%f' % (current_label, (current_sum * 1.0)/ current_count)
[training@localhost ~]$ 

# This is local testing of map and reduce.  Expect only three records from the reducer

[training@localhost ~]$ python mrstreaming/avg-word-length-by-letter/mapper.py < /tmp/ulysses.txt | sort | python mrstreaming/avg-word-length-by-letter/reducer.py
early	4.375647
late	4.466427
middle	4.702699

# Look at three error conditions:
#    * mapper or reducer is not executable
#    * syntax error in the reducer
#    * mapper throws an exception
#
# Transcript and errors deleted.  The thing to remember is how to go to the 
#  job tracker, look for failed task attempts, and examine the STDERR file for a
#  worker node that has a failed task.

# Here is a working version to show where the output data goes and how to look at it

[training@localhost ~]$ stream avg-word-length-by-letter ulysses ulysses
20/01/13 20:08:50 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
-- output removed except for the interesting lines-- 
20/01/13 20:10:11 INFO mapreduce.Job: Job job_1578944364430_0001 completed successfully
20/01/13 20:10:12 INFO streaming.StreamJob: Output directory: /user/hanks/data-output/ulysses

# Download and view the output files 
[training@localhost ~]$ hdfs dfs -getmerge /user/training/data-output/ulysses ulysses-count.txt
[training@localhost ~]$ cat ulysses-count.txt 
early	4.375647
late	4.466427
middle	4.702699
[training@localhost ~]$ 

############################################################################################
#  2.  Sqoop and relational databases

# Get a database into our MySQL server.  Start with a MySQL dump file then read it in to local MySQL

[training@localhost ~]$ mysqldump --host relational.fit.cvut.cz --port 3306 --user guest > /tmp/world.sql
[training@localhost ~]$ head /tmp/world.sql -n 5
-- MySQL dump 10.13  Distrib 5.1.73, for redhat-linux-gnu (x86_64)
--
-- Host: relational.fit.cvut.cz    Database: world
-- ------------------------------------------------------
-- Server version	5.5.5-10.3.15-MariaDB-log

# Create a new database for this database and load it in. 
[training@localhost ~]$ mysql -p
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> create database world;
Query OK, 1 row affected (0.00 sec)

mysql> quit
Bye
[training@localhost ~]$ mysql -p < /tmp/world.sql world
Enter password: 
[training@localhost ~]$ mysql -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1047
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| hue                |
| loudacre           |
| metastore          |
| mysql              |
| test               |
| world              |
+--------------------+
7 rows in set (0.01 sec)

mysql> use world;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+-----------------+
| Tables_in_world |
+-----------------+
| City            |
| Country         |
| CountryLanguage |
+-----------------+
3 rows in set (0.00 sec)

mysql> describe City;
+-------------+----------+------+-----+---------+----------------+
| Field       | Type     | Null | Key | Default | Extra          |
+-------------+----------+------+-----+---------+----------------+
| ID          | int(11)  | NO   | PRI | NULL    | auto_increment |
| Name        | char(35) | NO   |     |         |                |
| CountryCode | char(3)  | NO   | MUL |         |                |
| District    | char(20) | NO   |     |         |                |
| Population  | int(11)  | NO   |     | 0       |                |
+-------------+----------+------+-----+---------+----------------+
5 rows in set (0.00 sec)

### This is what we're going to do first in MapReduce:  what are the 10 countries with the highest
### population per city?   Easy to do in local SQL;  next do it in MapReduce -- a lot like average word length!

mysql> select CountryCode, avg(Population) from City group by CountryCode order by avg(Population) desc limit 10;
+-------------+-----------------+
| CountryCode | avg(Population) |
+-------------+-----------------+
| SGP         |    4017733.0000 |
| HKG         |    1650316.5000 |
| URY         |    1236000.0000 |
| GIN         |    1090610.0000 |
| UGA         |     890800.0000 |
| SLE         |     850000.0000 |
| LBR         |     850000.0000 |
| MLI         |     809552.0000 |
| AUS         |     808119.0000 |
| MNG         |     773700.0000 |
+-------------+-----------------+
10 rows in set (0.01 sec)

###  Task is to move the City table into HDFS then write a MapReduce script to do the average pop

[training@localhost ~]$ sqoop list-databases --connect jdbc:mysql://localhost/ --username training --password training
20/01/13 20:38:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3
20/01/13 20:38:27 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
20/01/13 20:38:28 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
information_schema
hue
loudacre
metastore
mysql
test
world

#  Notice how we specify a database in the connect URL

[training@localhost ~]$ sqoop list-tables --connect jdbc:mysql://localhost/world --username training --password training
20/01/13 20:39:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3
20/01/13 20:39:18 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
20/01/13 20:39:19 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
City
Country
CountryLanguage

#  This puts the City table in the directory /user/training/
#  It's worth looking at the MapReduce output to see how it's managing the import

[training@localhost ~]$ sqoop import --connect jdbc:mysql://localhost/world --username training --password training --table City --target-dir /user/training/data-input/city
20/01/14 19:38:55 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3
20/01/14 19:38:55 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
20/01/14 19:38:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
20/01/14 19:38:56 INFO tool.CodeGenTool: Beginning code generation
20/01/14 19:38:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `City` AS t LIMIT 1
20/01/14 19:38:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `City` AS t LIMIT 1
20/01/14 19:38:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-training/compile/f244c971b01030a3e8d7c97e9a892c10/City.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
20/01/14 19:39:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-training/compile/f244c971b01030a3e8d7c97e9a892c10/City.jar
20/01/14 19:39:03 WARN manager.MySQLManager: It looks like you are importing from mysql.
20/01/14 19:39:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
20/01/14 19:39:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
20/01/14 19:39:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
20/01/14 19:39:03 INFO mapreduce.ImportJobBase: Beginning import of City
20/01/14 19:39:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
20/01/14 19:39:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
20/01/14 19:39:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
20/01/14 19:39:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/01/14 19:39:10 INFO db.DBInputFormat: Using read commited transaction isolation
20/01/14 19:39:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `City`
20/01/14 19:39:10 INFO mapreduce.JobSubmitter: number of splits:4
20/01/14 19:39:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1578944364430_0009
20/01/14 19:39:12 INFO impl.YarnClientImpl: Submitted application application_1578944364430_0009
20/01/14 19:39:12 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1578944364430_0009/
20/01/14 19:39:12 INFO mapreduce.Job: Running job: job_1578944364430_0009
20/01/14 19:39:28 INFO mapreduce.Job: Job job_1578944364430_0009 running in uber mode : false
20/01/14 19:39:28 INFO mapreduce.Job:  map 0% reduce 0%
20/01/14 19:39:41 INFO mapreduce.Job:  map 25% reduce 0%
20/01/14 19:39:53 INFO mapreduce.Job:  map 50% reduce 0%
20/01/14 19:40:07 INFO mapreduce.Job:  map 75% reduce 0%
20/01/14 19:40:21 INFO mapreduce.Job:  map 100% reduce 0%
20/01/14 19:40:21 INFO mapreduce.Job: Job job_1578944364430_0009 completed successfully
20/01/14 19:40:21 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=543648
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=414
		HDFS: Number of bytes written=144481
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=4
		Other local map tasks=4
		Total time spent by all maps in occupied slots (ms)=0
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=44463
		Total vcore-seconds taken by all map tasks=44463
		Total megabyte-seconds taken by all map tasks=11382528
	Map-Reduce Framework
		Map input records=4079
		Map output records=4079
		Input split bytes=414
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=687
		CPU time spent (ms)=7380
		Physical memory (bytes) snapshot=489148416
		Virtual memory (bytes) snapshot=3378053120
		Total committed heap usage (bytes)=191889408
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=144481
20/01/14 19:40:21 INFO mapreduce.ImportJobBase: Transferred 141.0947 KB in 74.8397 seconds (1.8853 KB/sec)
20/01/14 19:40:21 INFO mapreduce.ImportJobBase: Retrieved 4079 records.

#  What are the output files, and what do they look like?

[training@localhost ~]$ hdfs dfs -ls /user/training/data-input/city
Found 5 items
-rw-rw-rw-   1 training supergroup          0 2020-01-14 19:40 /user/training/data-input/city/_SUCCESS
-rw-rw-rw-   1 training supergroup      37088 2020-01-14 19:39 /user/training/data-input/city/part-m-00000
-rw-rw-rw-   1 training supergroup      35361 2020-01-14 19:39 /user/training/data-input/city/part-m-00001
-rw-rw-rw-   1 training supergroup      35884 2020-01-14 19:40 /user/training/data-input/city/part-m-00002
-rw-rw-rw-   1 training supergroup      36148 2020-01-14 19:40 /user/training/data-input/city/part-m-00003

[training@localhost ~]$ hdfs dfs -cat /user/training/data-input/city/part-m-00000 | head -n 5
1,Kabul,AFG,Kabol,1780000
2,Qandahar,AFG,Qandahar,237500
3,Herat,AFG,Herat,186800
4,Mazar-e-Sharif,AFG,Balkh,127800
5,Amsterdam,NLD,Noord-Holland,731200
cat: Unable to write to output stream.

# Now with the city table we can recreate the query we did in MySQL:  compute the average population 
# per city for each country, and show the codes and average population for the top 10.  Our MapReduce 
# results should match MySQL
#
# This code is very similar to average number of letters per word category, so I started there and
# adapted the code only to reflect the different input records to the mapper, and the different field
# delimiter in the input file.

# Here is the code.  (Fair note .... my actual process here was to write the code, then run the local tests
# below, and I had to edit the code several times before getting a final correct version.)

[training@localhost ~]$ cat mrstreaming/avg-pop-per-city/mapper.py
#!/usr/bin/env python

import sys
import string

# Input records look like this:
#   1,Kabul,AFG,Kabol,1780000
# The third field is the country code, the fifth field is the population.
# We emit (code, population)
#
# Notice unlike word count, we are no emitting one record per word;
# we are just emitting one record per line. So the inner for loop goes away
#

for line in sys.stdin:
    fields = line.strip().split(",")
    country_code = fields[2]
    population = fields[4]
    print '%s\t%s' % (country_code, population)

[training@localhost ~]$ cat mrstreaming/avg-pop-per-city/reducer.py
#!/usr/bin/env python
import sys

# Input is tab-delimited tuples of the form (code, population)
# Taking the average population per code is exactly like average
# word length per word category

current_code = None
current_sum = 0
current_count = 0
code = None

for line in sys.stdin:
    line = line.strip()
    code, pop = line.split('\t')

    if current_code == code:
        current_sum += float(pop)
        current_count += 1
    else:
        if current_code:
            print '%s\t%f' % (current_code, current_sum/current_count)
        current_code = code
        current_sum = float(pop)
        current_count = 1

if current_code == code:
    print '%s\t%f' % (current_code, current_sum / current_count)

# Now here are some local tests of the mapper and the reducer.  To test, I'll download
# just one part file from the output table in HDFS

[training@localhost ~]$ hdfs dfs -get /user/training/data-input/city/part-m-00000
[training@localhost ~]$ head -n 2 part-m-00000 
1,Kabul,AFG,Kabol,1780000
2,Qandahar,AFG,Qandahar,237500

# Test the mapper alone.   Should be two-tuples, tab separated, country code then population

[training@localhost ~]$ python mrstreaming/avg-pop-per-city/mapper.py < part-m-00000 | head -n 5
AFG	1780000
AFG	237500
AFG	186800
AFG	127800
NLD	731200

# Test the mapper and reducer.  Should be two-tuples, tab separated, country code then a float (avg population)
[training@localhost ~]$ python mrstreaming/avg-pop-per-city/mapper.py < part-m-00000 | sort | python mrstreaming/avg-pop-per-city/reducer.py
ABW	29034.000000
AFG	583025.000000
AGO	512320.000000
AIA	778.000000
ALB	270000.000000
AND	21189.000000
ANT	2345.000000
ARE	345667.200000
ARG	350816.894737
ARM	544366.666667
ASM	3761.500000
ATG	24000.000000
AUS	808119.000000
AZE	616000.000000
BDI	300000.000000
BEL	178813.555556
BEN	242125.750000
BFA	409666.666667
BGD	357079.416667
BGR	269691.500000
BHR	148000.000000
BHS	172000.000000
BIH	199702.000000
BLZ	31457.500000
BMU	1500.000000
BOL	422330.500000
BRA	343507.448000
BRB	6070.000000
BRN	21484.000000
BTN	22000.000000
BWA	157411.000000
CHL	335102.413793
COK	11900.000000
CRI	339131.000000
CYM	19600.000000
DJI	383000.000000
DMA	16243.000000
DOM	406379.333333
DZA	288454.388889
ECU	382942.800000
EGY	542785.918919
ERI	431000.000000
ESP	282528.627119
ETH	455762.000000
FJI	77366.000000
FLK	1636.000000
FRO	14542.000000
GAB	419000.000000
GBR	276995.962963
GEO	376180.000000
GHA	363977.800000
GIB	27025.000000
GIN	1090610.000000
GLP	37690.000000
GMB	72463.000000
GNB	241000.000000
GRD	4621.000000
GRL	13445.000000
GTM	306297.000000
GUM	5319.500000
GUY	254000.000000
HKG	1650316.500000
HND	429000.000000
HTI	379334.500000
IDN	453838.963415
NLD	185001.750000
PHL	227461.698529
SJM	1438.000000
SLV	162604.428571
VGB	8000.000000
Traceback (most recent call last):
  File "mrstreaming/avg-pop-per-city/reducer.py", line 28, in <module>
    print '%s\t%f' % (current_label, current_sum / current_count)
NameError: name 'current_label' is not defined
[training@localhost ~]$ !!
python mrstreaming/avg-pop-per-city/mapper.py < part-m-00000 | sort | python mrstreaming/avg-pop-per-city/reducer.py | head -n 5
ABW	29034.000000
AFG	583025.000000
AGO	512320.000000
AIA	778.000000
ALB	270000.000000

###########################################
#  Now we're ready to run the whole job in Hadoop.   We should be all set:  mapper and reducer is in the 
#  right place, the input files are in the right place, we need to specify a name for the directory holding
#  the output files.   Remember the three arguments to our script specify location of scripts, location of input, 
#  location of output.

[training@localhost ~]$ stream avg-pop-per-city city city
rm: `/user/training/data-output/city/*': No such file or directory
rmdir: `/user/training/data-output/city': No such file or directory
20/01/15 09:24:37 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [/home/training/mrstreaming/avg-pop-per-city/mapper.py, /home/training/mrstreaming/avg-pop-per-city/reducer.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.4.3.jar] /tmp/streamjob6433550570484430623.jar tmpDir=null
20/01/15 09:24:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/01/15 09:24:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/01/15 09:24:39 INFO mapred.FileInputFormat: Total input paths to process : 4
20/01/15 09:24:39 INFO mapreduce.JobSubmitter: number of splits:4
20/01/15 09:24:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1578944364430_0010
20/01/15 09:24:40 INFO impl.YarnClientImpl: Submitted application application_1578944364430_0010
20/01/15 09:24:40 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1578944364430_0010/
20/01/15 09:24:40 INFO mapreduce.Job: Running job: job_1578944364430_0010
20/01/15 09:24:47 INFO mapreduce.Job: Job job_1578944364430_0010 running in uber mode : false
20/01/15 09:24:47 INFO mapreduce.Job:  map 0% reduce 0%
20/01/15 09:24:53 INFO mapreduce.Job:  map 25% reduce 0%
20/01/15 09:24:58 INFO mapreduce.Job:  map 50% reduce 0%
20/01/15 09:25:03 INFO mapreduce.Job:  map 75% reduce 0%
20/01/15 09:25:08 INFO mapreduce.Job:  map 100% reduce 0%
20/01/15 09:25:14 INFO mapreduce.Job:  map 100% reduce 100%
20/01/15 09:25:14 INFO mapreduce.Job: Job job_1578944364430_0010 completed successfully
20/01/15 09:25:15 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=52701
		FILE: Number of bytes written=678035
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=144945
		HDFS: Number of bytes written=4069
		HDFS: Number of read operations=15
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=4
		Launched reduce tasks=1
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=0
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=15503
		Total time spent by all reduce tasks (ms)=3970
		Total vcore-seconds taken by all map tasks=15503
		Total vcore-seconds taken by all reduce tasks=3970
		Total megabyte-seconds taken by all map tasks=3968768
		Total megabyte-seconds taken by all reduce tasks=2032640
	Map-Reduce Framework
		Map input records=4079
		Map output records=4079
		Map output bytes=44537
		Map output materialized bytes=52719
		Input split bytes=464
		Combine input records=0
		Combine output records=0
		Reduce input groups=232
		Reduce shuffle bytes=52719
		Reduce input records=4079
		Reduce output records=232
		Spilled Records=8158
		Shuffled Maps =4
		Failed Shuffles=0
		Merged Map outputs=4
		GC time elapsed (ms)=296
		CPU time spent (ms)=3110
		Physical memory (bytes) snapshot=989696000
		Virtual memory (bytes) snapshot=4416163840
		Total committed heap usage (bytes)=644481024
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=144481
	File Output Format Counters 
		Bytes Written=4069
20/01/15 09:25:15 INFO streaming.StreamJob: Output directory: /user/training/data-output/city

##################################################
# Job completed correctly.  Check whether output files are there, and look OK

[training@localhost ~]$ hdfs dfs -ls /user/training/data-output/city
Found 2 items
-rw-rw-rw-   1 training supergroup          0 2020-01-15 09:25 /user/training/data-output/city/_SUCCESS
-rw-rw-rw-   1 training supergroup       4069 2020-01-15 09:25 /user/training/data-output/city/part-00000
[training@localhost ~]$ hdfs dfs -cat  /user/training/data-output/city/part-00000 | head -n 5
ABW	29034.000000
AFG	583025.000000
AGO	512320.000000
AIA	778.000000
ALB	270000.000000

##########################################################################
#  One last integrity check.   The number of rows in the 
#  output file should be the number of distinct country codes in the city table in 
#  SQL

[training@localhost ~]$ mysql -p world
mysql> select count(distinct(CountryCode)) from City;
+------------------------------+
| count(distinct(CountryCode)) |
+------------------------------+
|                          232 |
+------------------------------+
1 row in set (0.00 sec)

mysql> quit
Bye
[training@localhost ~]$ hdfs dfs -cat  /user/training/data-output/city/part-00000 | wc -l
232

####################################################################
#  Now to finish it off.  We don't (yet) know how to do the sort in MapReduce, 
#  but we can do it using Unix tools:  sort descending on the second field (avg population)
#  then choose the first 10.
#  This output should exactly match the result of the MySQL query we did on the world database.

[training@localhost ~]$ hdfs dfs -cat  /user/training/data-output/city/part-00000 | sort -k 2 -n -r | head -n 10
SGP	4017733.000000
HKG	1650316.500000
URY	1236000.000000
GIN	1090610.000000
UGA	890800.000000
SLE	850000.000000
LBR	850000.000000
MLI	809552.000000
AUS	808119.000000
MNG	773700.000000

########################################################################
##  To finish the work on Sqoop I want to:
##    * use Sqoop to move information from HDFS into a database table (export)
##  Will just put the average population per country back into the "world" database.
##  You wouldn't ordinarily do that because that data is the result of a query on those
##  tables, so why bother?  But the example is just to demonstrate Sqoop export.

## Create the MySQL table first
[training@localhost ~]$ mysql -ptraining world
mysql> create table AveragePopulationPerCountry (CountryCode char(3), AveragePopulation float);
Query OK, 0 rows affected (0.03 sec)
mysql> quit
Bye

## Export from HDFS output directory to the new table.  Notice we have to let Sqoop know that fields are tab delimited!
[training@localhost ~]$ sqoop export --connect jdbc:mysql://localhost/world --username training --password training --table AveragePopulationPerCountry --export-dir /user/training/data-output/city --input-fields-terminated-by "\t"
20/01/15 10:10:39 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3
20/01/15 10:10:39 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
20/01/15 10:10:39 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
20/01/15 10:10:39 INFO tool.CodeGenTool: Beginning code generation
20/01/15 10:10:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `AveragePopulationPerCountry` AS t LIMIT 1
20/01/15 10:10:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `AveragePopulationPerCountry` AS t LIMIT 1
20/01/15 10:10:40 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-training/compile/771de167192f7942c979a75fca1bd823/AveragePopulationPerCountry.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
20/01/15 10:10:42 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-training/compile/771de167192f7942c979a75fca1bd823/AveragePopulationPerCountry.jar
20/01/15 10:10:42 INFO mapreduce.ExportJobBase: Beginning export of AveragePopulationPerCountry
20/01/15 10:10:42 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
20/01/15 10:10:42 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
20/01/15 10:10:43 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
20/01/15 10:10:43 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
20/01/15 10:10:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
20/01/15 10:10:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
20/01/15 10:10:45 INFO input.FileInputFormat: Total input paths to process : 1
20/01/15 10:10:45 INFO input.FileInputFormat: Total input paths to process : 1
20/01/15 10:10:45 INFO mapreduce.JobSubmitter: number of splits:4
20/01/15 10:10:45 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
20/01/15 10:10:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1578944364430_0011
20/01/15 10:10:45 INFO impl.YarnClientImpl: Submitted application application_1578944364430_0011
20/01/15 10:10:45 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1578944364430_0011/
20/01/15 10:10:45 INFO mapreduce.Job: Running job: job_1578944364430_0011
20/01/15 10:10:54 INFO mapreduce.Job: Job job_1578944364430_0011 running in uber mode : false
20/01/15 10:10:54 INFO mapreduce.Job:  map 0% reduce 0%
20/01/15 10:11:00 INFO mapreduce.Job:  map 25% reduce 0%
20/01/15 10:11:05 INFO mapreduce.Job:  map 50% reduce 0%
20/01/15 10:11:10 INFO mapreduce.Job:  map 75% reduce 0%
20/01/15 10:11:16 INFO mapreduce.Job:  map 100% reduce 0%
20/01/15 10:11:16 INFO mapreduce.Job: Job job_1578944364430_0011 completed successfully
20/01/15 10:11:16 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=542576
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=11354
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=0
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=15612
		Total vcore-seconds taken by all map tasks=15612
		Total megabyte-seconds taken by all map tasks=3996672
	Map-Reduce Framework
		Map input records=232
		Map output records=232
		Input split bytes=656
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=277
		CPU time spent (ms)=2130
		Physical memory (bytes) snapshot=480694272
		Virtual memory (bytes) snapshot=3369631744
		Total committed heap usage (bytes)=191889408
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
20/01/15 10:11:16 INFO mapreduce.ExportJobBase: Transferred 11.0879 KB in 32.4739 seconds (349.635 bytes/sec)
20/01/15 10:11:16 INFO mapreduce.ExportJobBase: Exported 232 records.
[training@localhost ~]$ mysql -ptraining world
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3798
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

##  And make sure 
[training@localhost ~]$ mysql -ptraining world
mysql> select * from AveragePopulationPerCountry order by AveragePopulation desc limit 10;
+-------------+-------------------+
| CountryCode | AveragePopulation |
+-------------+-------------------+
| SGP         |       4.01773e+06 |
| HKG         |       1.65032e+06 |
| URY         |         1.236e+06 |
| GIN         |       1.09061e+06 |
| UGA         |            890800 |
| LBR         |            850000 |
| SLE         |            850000 |
| MLI         |            809552 |
| AUS         |            808119 |
| MNG         |            773700 |
+-------------+-------------------+
10 rows in set (0.00 sec)

