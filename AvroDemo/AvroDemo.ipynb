{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avro Demo\n",
    "\n",
    "Situation so far:\n",
    "\n",
    "* Process streams of records using MapReduce (or MapReduce streaming)\n",
    "\n",
    "* Input data is from either delimited text files or \"free text\" files in HDFS\n",
    "  * Most data sets have a lot of structure -- they look more like rows in DB tables than like a flat sequence of fields\n",
    "\n",
    "*  What is Structure / Schema Information?\n",
    "  *  Fields have a type, types require validation\n",
    "  *  Fields are optional or mandatory\n",
    "  *  Aggregate fields:  arrays and records\n",
    "  \n",
    "* Fundamental operation performed by all mappers\n",
    "  * Read the record\n",
    "  * Parse into field on delimiter\n",
    "  * Validate fields (data type, mandatory fields present)\n",
    "  * Access fields to do calculation\n",
    "\n",
    "* Limitations due to flat records\n",
    "  * Fields are defined (only) by position\n",
    "    * Not natural / readable, so error prone\n",
    "  * Code to do record parsing must be repeated every place the record is used\n",
    "    * Having the same code in multiple places is never good software engineering\n",
    "    \n",
    "  * Changes to the schema will tend to break code (everywhere)\n",
    "    * Change field delimiter\n",
    "    * Change field order\n",
    "    * Add new fields\n",
    "    * Change data type\n",
    "    * Delete unneeded fields\n",
    "  \n",
    "Ideally:\n",
    "* The data schema lives with the data or is centrally defined, not redundantly defined with the process code\n",
    "* Processors can use an \"object-like\" representation for their code\n",
    "  * More readable, not dependent on actual format of record\n",
    "* Processors react \"appropriately\" in case the schema changes\n",
    "  * No code change required in case of non-breaking change\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data Set\n",
    "\n",
    "Embellished version of the City table\n",
    "\n",
    "Fields (in order):\n",
    "\n",
    "1.  id\n",
    "1.  name\n",
    "1.  country_code\n",
    "1.  district\n",
    "1.  up to three neighborhoods\n",
    "1.  mayor name\n",
    "1.  year mayor elected\n",
    "\n",
    "Schema requirements:\n",
    "\n",
    "* id is required and must be long\n",
    "* name is required and must be non-empty\n",
    "* country_code is required and must be a known country code\n",
    "* district is optional\n",
    "* up to three neighborhoods, all optional\n",
    "* mayor name is required but date elected is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = [\"USA\", \"NLD\", \"AFG\"]\n",
    "\n",
    "def read_cities():\n",
    "    cities = []\n",
    "    with open(\"city.txt\") as cityfile:\n",
    "        for line in cityfile:\n",
    "            cities.append(read_city(line))\n",
    "    return cities\n",
    "        \n",
    "def read_city(line):\n",
    "    id, name, country_code, \\\n",
    "        district, population, \\\n",
    "        n1, n2, n3, \\\n",
    "        mayor_name, year_elected = line.strip().split(\",\")\n",
    "         \n",
    "    # Check mandatory fields\n",
    "    if (not id) or (not name) or (not mayor_name):\n",
    "            raise Exception(\"Missing a field\")\n",
    "            \n",
    "    # Check valid country code   \n",
    "    if country_code not in country_codes:\n",
    "        raise Exception(f\"Bad country code {country_code}\")\n",
    "        \n",
    "    # Parse and convert integer fields\n",
    "    id = int(id)\n",
    "    population = int(population)\n",
    "        \n",
    "    # Possibly three neighborhoods, but could be in \n",
    "    # any of the three input fields\n",
    "    neighborhoods = []\n",
    "    if (n1):\n",
    "        neighborhoods.append(n1)\n",
    "    if (n2):\n",
    "        neighborhoods.append(n2)\n",
    "    if (n3):\n",
    "        neighborhoods.append(n3)\n",
    "        \n",
    "    # Mayor is a structure with mandatory name\n",
    "    # and optional date elected\n",
    "        \n",
    "    mayor = {\"name\": mayor_name}\n",
    "    if year_elected:\n",
    "        mayor[\"year_elected\"] = int(year_elected) if year_elected else None\n",
    "    \n",
    "    city = {\"id\": id, \n",
    "            \"country_code\": country_code,\n",
    "            \"name\": name, \n",
    "            \"district\": district,\n",
    "            \"neighborhoods\": neighborhoods,\n",
    "            \"mayor\": mayor }\n",
    "        \n",
    "    return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  How does this related to MapReduce / HDFS?\n",
    "json_cities = read_cities()\n",
    "print(json_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now view the Avro schema for this record type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import avro.schema\n",
    "\n",
    "# First just parse the schema to see if the declaration is OK syntax\n",
    "schema = avro.schema.Parse(open(\"cityv1.avsc\", \"rb\").read())\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already converted our data to JSON -- now we can write an Avro file with the same data\n",
    "\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter\n",
    "\n",
    "json_cities = read_cities()\n",
    "schema = avro.schema.Parse(open(\"cityv1.avsc\", \"rb\").read())\n",
    "with DataFileWriter(open(\"cities.avro\", \"wb\"), DatumWriter(), schema) as writer:\n",
    "    for record in json_cities:\n",
    "        writer.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ??  What happens if the data doesn't conform to the schema??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the payoff!  Does our life get easier if the file is in Avro?\n",
    "#Simpler, no splitting no parsing.\n",
    "\n",
    "with DataFileReader(open(\"cities_v1.avro\", \"rb\"), DatumReader()) as reader:\n",
    "    for city in reader:\n",
    "        print(city['name'] + \", \" + \\\n",
    "              city['country_code'] + \", \" + \\\n",
    "              city['mayor']['name'] + \", \" + \n",
    "              str(len(city['neighborhoods'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to MapReduce\n",
    "\n",
    "Remember average population per city grouped by country code.  Here is the mapper and reducer.\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "\"\"\"mapper.py\"\"\"\n",
    "\n",
    "import sys\n",
    "import string\n",
    "\n",
    "# Input records look like this:\n",
    "#   1,Kabul,AFG,Kabol,1780000\n",
    "# The third field is the country code, the fifth field is the population.\n",
    "# We emit (code, population)\n",
    "#\n",
    "# Notice unlike word count, we are no emitting one record per word;\n",
    "# we are just emitting one record per line. So the inner for loop goes away\n",
    "#\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split(\",\")\n",
    "    country_code = fields[2]\n",
    "    population = fields[4]\n",
    "    print '%s\\t%s' % (country_code, population)\n",
    "\n",
    "#########################################################\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "# Input is tab-delimited tuples of the form (code, population)\n",
    "# Taking the average population per code is exactly like average\n",
    "# word length per word category\n",
    "\n",
    "current_code = None\n",
    "current_sum = 0\n",
    "current_count = 0\n",
    "code = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    code, pop = line.split('\\t', 1)\n",
    "\n",
    "    if current_code == code:\n",
    "        current_sum += float(pop)\n",
    "        current_count += 1\n",
    "    else:\n",
    "        if current_code:\n",
    "            print '%s\\t%f' % (current_code, current_sum/current_count)\n",
    "        current_code = code\n",
    "        current_sum = float(pop)\n",
    "        current_count = 1\n",
    "\n",
    "if current_code == code:\n",
    "    print '%s\\t%f' % (current_code, current_sum / current_count)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the mapper and reducer change when we switch to reading from Avro rather than text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Changes\n",
    "\n",
    "The schema changes when \n",
    "1. We start buying data from a new provider that supplies different records\n",
    "2. Upstream calculation starts supplying different information\n",
    "\n",
    "In that case\n",
    "* The \"writer\" of the Avro data changes from v1 to v2\n",
    "* Do all of the \"readers\" of the Avro data have to adopt v2, or if they want, can they continue to process the data using v1 schema?\n",
    "\n",
    "This makes a huge difference:  suppose you have *many* readers company-wide, only some of them care about the v2 data.\n",
    "* Does the whole company need to simultaneously switch to v2?\n",
    "* If so, there is so much pain to be had!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Change Example\n",
    "\n",
    "Our data v2 has the following changes\n",
    "* A new int attribute **area**\n",
    "* Demote the **id** attribute from **long** to **int**\n",
    "* Remove the mistaken country code \"XYZ\"\n",
    "\n",
    "Can our old application still process data written with the new schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First let's write some v2 data\n",
    "\n",
    "new_city_data = [\n",
    " {'id': 9001, 'name': 'Portland', 'area': 145, 'country_code': 'USA', \n",
    "  'neighborhoods': ['Portland Heights'], 'mayor': {'name': 'Ted Wheeler'}},   \n",
    "  {'id': 9002, 'name': 'Vancouver', 'country_code': 'CAN', 'area': 44,\n",
    "   'neighborhoods': [], 'mayor': {'name': 'Kennedy Stuart', 'year_elected': 2018}}\n",
    "]\n",
    "\n",
    "schema = avro.schema.Parse(open(\"cityv2.avsc\", \"rb\").read())\n",
    "\n",
    "with DataFileWriter(open(\"cities_v2.avro\", \"wb\"), DatumWriter(), schema) as writer:\n",
    "    for record in new_city_data:\n",
    "        writer.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the case of an v1 application \n",
    "## -- that knows only about the v1 schema --\n",
    "## processing data written with the v2 schema\n",
    "\n",
    "reader_schema = avro.schema.Parse(open(\"cityv1.avsc\", \"rb\").read()) \n",
    "writer_schema = avro.schema.Parse(open(\"cityv2.avsc\", \"rb\").read())\n",
    "\n",
    "with DataFileReader(open(\"cities_v2.avro\", \"rb\"), DatumReader(writer_schema, reader_schema)) as reader:\n",
    "    for city in reader:\n",
    "        print(city['name'] + \", \" +  city['country_code'] + \", \" + city['mayor']['name'] + \", \" + str(len(city['neighborhoods'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a third schema where the mandatory attribute id is removed \n",
    "v3_cities = [\n",
    " {'name': 'Bend', 'area': 145, 'country_code': 'USA', \n",
    "  'neighborhoods': [], 'mayor': {'name': 'Sally Russell'}}\n",
    "]\n",
    "\n",
    "schemav3 = avro.schema.Parse(open(\"cityv3.avsc\", \"rb\").read())\n",
    "\n",
    "with DataFileWriter(open(\"cities_v3.avro\", \"wb\"), DatumWriter(), schemav3) as writer:\n",
    "    for record in v3_cities:\n",
    "        writer.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_schema = avro.schema.Parse(open(\"cityv1.avsc\", \"rb\").read()) \n",
    "writer_schema = avro.schema.Parse(open(\"cityv3.avsc\", \"rb\").read())\n",
    "\n",
    "with DataFileReader(open(\"cities_v3.avro\", \"rb\"), DatumReader(writer_schema, reader_schema)) as reader:\n",
    "    for city in reader:\n",
    "        print(city['name'] + \", \" +  city['country_code'] + \", \" + city['mayor']['name'] + \", \" + str(len(city['neighborhoods'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
