LAB NOTES FOR Feb 10

THIS LAB WILL NOT BE HANDED IN

We are in the process of moving off the VM environment and into a real cluster / distributed data pipeline on AWS.

In the previous lab we looked at S3 to store files 
  *  input data files
  *  output of EMR jobs
  *  JAR files to run MapReduce, config files etc.

We also practiced at submitting a "custom JAR" EMR job through the EMR console.

In this lab we will continue exploring AWS with the idea of completely reconstructing the document search example on AWS.  So we will have EMR jobs to do the indexing followed by putting computed TFIDF results in Cassandra/
DynamoDB, followed by some kind of application to return query results.

Directions we go next:
  -- look at DynamoDB as our Cassandra equivalent
  -- create a DynamoDB table to hold our TFIDF data
  -- migrate TFIDF data to the DynamoDB tables
  -- write a Python application to retrieve from DynamoDB
  -- (think about what our end search application might look like)

==============
Explore DynamoDB on the AWS Console
Goals:
   * Learn the parameters and methods required for 
     creating a table and getting and putting items in the table
   * Create a table, get and put an item in the table 
        using the console

1.  Look at DynamoDB console on AWS page
2.  Understand how AWS charges for DynamoDB -- read Units and
       Write Units -- you can 
       get more performance by increasing RU and/or WU, 
       but be careful to set them back because you get charged
       for capacity whether you are using it or not.  

Read this for understanding provisioned throughput charging.  https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html


1. Create the TFIDF table for our TFIDF application -- start at the AWS DynamoDB Console.
2.  Remember that our TFIDF data is of the form
        <term>\t<docid>\t<tfidf-value> 
and eventually we need to load that data into a table.

  -- Create table -- give it any name you want, like tfidf
    -- partition key:  for our search application we will be
          retrieving by 'term'
    -- sort key:  this allows retrieval by ranges or value within
          a partition key.  For us that might be 'doc_id'
    -- secondary indexes: we will not use secondary indexes, 
          but read documentation to understand what secondary
          index are used for

3.  Once the table is created, use the GUI to add an item and query an item.  We will eventually want to 
do the adding and querying from a Python program but for now the GUI is good for understanding the concepts.

4.  In the GUI, look at other parameters for the table, like scaling / capacity.  That gives us an idea of 
what choices we might make if we were to use the table in production

AT THE END OF THIS PHASE
  -- Create a DynamoDB table
  -- Put a new item in the table, and lookup an item in the
     table, using the GUI

=============================
Now we have two use cases for the table:

1.  Move the TFIDF data currently in S3 to the dynamodb table
2.  Build a search/query application that does lookups in DynamoDB

The TFIDF data is in this location:

s3://hanks-bda-2020-01/data-output/termDocumentCount/

You should try to locate this bucket and folder in the AWS
Console.  If you cannot get access to the bucket, let me
know and we will figure out how you can move this data into
your own S3 bucket.

The data load is moving these records (term, docid, tfidf)
the DynamoDB table you just created.

There are good tools in AWS to do the data migration:

1.  Data Pipeline
2.  Database migration service

But our accounts are not enabled to use either of those, so we will have to write code to do it:
   
* A Python application to read S3 files and do PutItem into the DynamoDB database.

AT THIS PHASE:
   You know the location of an S3 bucket containing 
   the TFIDF output from Assignment 1.

============================================
CREATING an EC2 Instance

For both the indexing and query applications, it is best if we provision an EC2 instance -- that will give us a uniform platform, 
like our Cloudera VM only with more up to date software.
https://aws.amazon.com/ec2/

To create an EC2 instance, we need to specify AMI (Amazon Machine Image) and instance size.

1.  Go to the EC2 Console page and select Create Instance, and it will ask you for both AMI and instance type
For this class we will use
   * AMI Type:  ami-04b9e92b5572fa0d1  
     (Ubuntu -- has the right Linux version)
   * Instance type:  t2.micro

In order to log onto the instance, you will need a key pair.
Go to  EC2 -> Key Pairs  for creating a pair.
Read 
http://resources.intenseschool.com/amazon-aws-understanding-ec2-key-pairs-and-how-they-are-used-for-windows-and-linux-instances/

2.  Generate a key pair according to whether you are using Windows or MacOS.

3.  Launch the instance
   It is useful to give the instance a name for your own future
   reference
  
4.  SSH to the instance
The page for the instance will have Connect instructions -- different for Windows and Mac/Linux.
Verify that you can successfully log on to the instance.
Check that Python is present
    which python3

5.  Now that we can SSH to the instance and have a shell, the next step is to get CLI (Amazon command line interface) 
access to S3 and DynamoDB.
Read documentation on AWS CLI: https://aws.amazon.com/cli/

6.  Install the CLI on your EC2 instance

sudo apt-get update
sudo apt-get install awscli

7.  The CLI needs to be configured with security tokens 
associated with your  account.  
Read this material on configuring the CLI https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html

8.  You need to find your security credentials (Access Key and Secret Access Key).  The location of this information varies from account to account, but start at the AWS console, use the dropdown
showing your name/account at the top, and select "My Security Credentials."  then Access Keys.   You are looking for these
lines:


    Access Key ID:
    AKIAJ45ONKRLEUYAZAIQ
    Secret Access Key:
    yspYvFuAYzzcauXI0sFD7/cW9WXGOxTMUpCA0Ap1

You may have to create a new access key.  Save these keys.

You now need to configure CLI credentials on your EC2.
Credentials are stored in a file .aws/configure and the content
looks like this:

[default]
aws_access_key_id=ASIAQXPMTFWOJZK...
aws_secret_access_key=Q61leYbzpQTjbUNyu79W  ....
aws_session_token=FwoGZXIvYXdzEIn


though your account may not have a session token.
You can populate the values on the EC2 using the command

aws configure

or you can edit .aws/configure directly.  

You can search for 'configure aws cli' for 
more information.  For example:  https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html

After finishing the configuration, verify you have access to AWS by trying these commands:
  * aws s3 ls
  * aws dynamodb list-tables
 
The latter command should show the dynamoDB table you just created.

You should do more research about the commands available using the CLI.  For example, how do you do a DynamoDB query using the CLI? 

AT THIS PHASE
  * You can create an EC2 instance and log on to it 
     using a key pair
  * You have configued the AWS CLI with your credentials
  * You can run commands through the CLI to 
     * List your S3 buckets
     * List your dynamoDB tables

=============================
For next time:

We will use the boto3 library as a Python to AWS interface.

For reference, consult the boto3 documentation:  https://boto3.amazonaws.com/v1/documentation/api/latest/index.html

You will need to install boto3 on your EC2 instance
        pip install boto3
